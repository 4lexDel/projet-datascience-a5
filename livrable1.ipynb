{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 1 - groupe 1\n",
    "\n",
    "Nous sommes le groupe 1 composé de :  \n",
    "- DELORME Alexandre\n",
    "- ENCOGNERE Yanis\n",
    "- MENNERON Laurine\n",
    "- PEREON Alexandre\n",
    "- ROCHARD Léo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TODO\n",
    "\n",
    "##  Analyse des Résultats et Biais/VarianceAnalyse\n",
    "\n",
    "Discussion sur le surapprentissage/sous-apprentissage\n",
    "\n",
    "Calcul et affichage d'une matrice de confusion\n",
    "\n",
    "Calcul de métriques supplémentaires (précision, rappel, F1-score)\n",
    "\n",
    "## À implémenter/discuter:\n",
    "\n",
    "Régularisation L2 dans les couches denses/convolutionnelles\n",
    "\n",
    "Ajustement du taux de dropout\n",
    "\n",
    "Utilisation de Batch Normalization\n",
    "\n",
    "Transfer learning avec un modèle pré-entraîné (VGG16, ResNet, etc.)\n",
    "\n",
    "Grid search pour optimiser les hyperparamètres\n",
    "\n",
    "## Documentation et Justifications\n",
    "\n",
    "Schéma de l'architecture du réseau (peut être généré avec tf.keras.utils.plot_model)\n",
    "\n",
    "Justification des choix (architecture, hyperparamètres, etc.)\n",
    "\n",
    "Analyse des résultats obtenus\n",
    "\n",
    "Discussion sur les difficultés rencontrées (images réalistes dans les peintures)\n",
    "\n",
    "## Gestion des Données Déséquilibrées\n",
    "\n",
    "Analyse du déséquilibre entre classes photo/non-photo\n",
    "\n",
    "Techniques pour gérer le déséquilibre (poids de classe, oversampling, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%load_ext tensorboard\n",
    "import os\n",
    "import pathlib\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import visualkeras\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement du dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette partie permet de récuperer le dataset depuis le drive afin d'automatiser la pipeline. L'objectif est d'utiliser un dataset commun au sein du groupe. \n",
    "\n",
    "Ce dernier à le dossier \"Sketch\" complété par des données trouvés dans les sources suivantes : [croquis](https://paperswithcode.com/dataset/sketch) || [visages réalistes](https://www.kaggle.com/datasets/arbazkhan971/cuhk-face-sketch-database-cufs/data)\n",
    "\n",
    "Ce qui fait passer le dossier sketch de 606 visages et 800 croquis avec un ration de 43%/57% entre visages/croquis à 1200 visages et 3200 croquis pour un ratio final équivalent."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ID du fichier (extrait de l'URL)\n",
    "file_id = \"1d0k3mXd93JM0fLLYrUZRr4Un1F_TnuHi\"\n",
    "dataset_path = \"dataset_livrable_1\"\n",
    "zip_path = dataset_path + \".zip\"\n",
    "extract_dir = pathlib.Path(zip_path).parent / dataset_path\n",
    "reduce_dataset = True\n",
    "\n",
    "if not os.path.exists(extract_dir):\n",
    "    print(f\"Le dossier '{extract_dir}' n'existe pas. Téléchargement en cours...\")\n",
    "    gdown.download(f\"https://drive.google.com/uc?id={file_id}\", zip_path, quiet=False)\n",
    "\n",
    "    print(f\"Extraction ZIP en cours...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\"Extraction Zip terminée\")\n",
    "else:\n",
    "    print(f\"Le dossier '{extract_dir}' existe déjà. Téléchargement et extraction non nécessaires.\")\n",
    "\n",
    "data_dir = extract_dir\n",
    "print(f\"Dataset disponible dans : {data_dir}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Détection des catégories"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "categories = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "print(f\"Catégories détectées : {categories}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré traitement des images"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Suppresion des fichiers corrompus ou non images --------------------------------------------------------------------\n",
    "def clean_images_dataset(dataset_path_arg):\n",
    "    \"\"\"\n",
    "    Fonction pour nettoyer le dataset en supprimant les fichiers corrompus ou non images.\n",
    "    \"\"\"\n",
    "    # Dictionnaire pour stocker le nombre d'images corrompues par classe\n",
    "    corrupted_count_by_class = defaultdict(int)\n",
    "    dataset_path = dataset_path_arg\n",
    "    print(\"Début de la vérification des images ...\")\n",
    "\n",
    "    # Récupération de toutes les images pour calculer la progression\n",
    "    all_files = []\n",
    "    for dir_name in os.listdir(dataset_path):\n",
    "        dir_path = os.path.join(dataset_path, dir_name)\n",
    "        for file_name in os.listdir(dir_path):\n",
    "            all_files.append((dir_name, dir_path, file_name))\n",
    "\n",
    "    total_files = len(all_files)\n",
    "    checked_files = 0  # Pour la progression\n",
    "\n",
    "    # Parcours des images avec affichage de la progression\n",
    "    for dir_name, dir_path, file_name in all_files:\n",
    "        if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "            try:\n",
    "                with open(os.path.join(dir_path, file_name), 'rb') as file:\n",
    "                    img_bytes = file.read()  # Lire les bytes de l'image\n",
    "                    img = tf.image.decode_image(img_bytes)  # Essayer de décoder l'image\n",
    "            except Exception as e:\n",
    "                corrupted_count_by_class[dir_name] += 1\n",
    "                print(f\"\\nImage corrompue : {file_name} dans {dir_name}. Exception: {e}\")\n",
    "                os.remove(os.path.join(dir_path, file_name))\n",
    "                print(f\"Image {file_name} supprimée.\")\n",
    "        else:\n",
    "            corrupted_count_by_class[dir_name] += 1\n",
    "            print(f\"\\nLe fichier {file_name} dans {dir_name} n'est pas une image.\")\n",
    "            os.remove(os.path.join(dir_path, file_name))\n",
    "            print(f\"Fichier {file_name} supprimé.\")\n",
    "\n",
    "        # Mise à jour de la progression\n",
    "        checked_files += 1\n",
    "        progress = (checked_files / total_files) * 100\n",
    "        print(f\"\\rProgression : [{int(progress)}%] {checked_files}/{total_files} images vérifiées\", end=\"\")\n",
    "\n",
    "    print(\"\\nVérification des fichiers terminée.\")\n",
    "\n",
    "    # Affichage du nombre d'images corrompues par dossier\n",
    "    for dir_name, count in corrupted_count_by_class.items():\n",
    "        print(f\"Dossier {dir_name} : {count} images corrompues\")\n",
    "\n",
    "    # Nombre total d'images corrompues\n",
    "    total_corrupted = sum(corrupted_count_by_class.values())\n",
    "    print(f\"Nombre total d'images corrompues ou non image : {total_corrupted}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul du nombre de données par classe"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class_counts = {class_name: len(list((data_dir / class_name).glob('*'))) for class_name in categories}\n",
    "class_counts['Sketch'] = len(list((data_dir / 'Sketch' / 'jpg').glob('*'))) + len(\n",
    "    list((data_dir / 'Sketch' / 'png').glob('*')))\n",
    "print(class_counts)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des images"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "validation_split = 0.2\n",
    "seed = 42\n",
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "train_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=validation_split,\n",
    "    subset=\"training\",\n",
    "    seed=seed,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_names=categories)\n",
    "\n",
    "val_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=validation_split,\n",
    "    subset=\"validation\",\n",
    "    seed=seed,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_names=categories)\n",
    "\n",
    "class_names = train_set.class_names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"Classes found: {class_names}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des différentes classes"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "for images, labels in train_set.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Répartition des classes au sein du dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(class_counts.keys(), class_counts.values(), color='skyblue')\n",
    "plt.title(\"Class Repartition\")\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Number of Samples\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Préparation pour analyse\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Convertir les images et labels en arrays\n",
    "images, labels = zip(*[(image_batch.numpy(), label_batch.numpy()) for image_batch, label_batch in train_set])\n",
    "images = np.concatenate(images)\n",
    "labels = np.concatenate(labels)\n",
    "\n",
    "# Sélectionner 20% des données aléatoirement\n",
    "num_samples = images.shape[0]\n",
    "sample_size = int(num_samples * 0.20)  # 20% du dataset\n",
    "indices = np.random.choice(num_samples, sample_size, replace=False)\n",
    "\n",
    "# Extraire les échantillons sélectionnés\n",
    "images_sampled = images[indices]\n",
    "labels_sampled = labels[indices]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Analyse en composantes principales\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Réduction dimensionnelle avec PCA\n",
    "images_reshaped = images_sampled.reshape(images_sampled.shape[0], -1)\n",
    "pca = PCA(n_components=0.95)\n",
    "pca_result = pca.fit_transform(images_reshaped)\n",
    "\n",
    "# Affichage PCA\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(len(class_names)):\n",
    "    plt.scatter(pca_result[labels_sampled == i, 0], pca_result[labels_sampled == i, 1], label=class_names[i], alpha=0.4)\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"2D PCA Plot of Image Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "x_min = np.round(pca_result[:, 0].min(), -4)\n",
    "x_max = np.round(pca_result[:, 0].max(), -4)\n",
    "y_min = np.round(pca_result[:, 1].min(), -4)\n",
    "y_max = np.round(pca_result[:, 1].max(), -4)\n",
    "\n",
    "# Créer des subplots avec 2 lignes et 3 colonnes (ajuster en fonction du nombre de classes)\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n",
    "axes = axes.flatten()  # Aplatir la matrice d'axes pour un accès facile\n",
    "\n",
    "# Affichage de PCA pour chaque classe dans un subplot distinct\n",
    "for i in range(len(class_names)):\n",
    "    ax = axes[i]  # Sélectionner l'axe pour la classe i\n",
    "\n",
    "    # Filtrer les points de la classe i\n",
    "    ax.scatter(pca_result[labels_sampled == i, 0], pca_result[labels_sampled == i, 1],\n",
    "               alpha=0.6, label=f\"{class_names[i]}\", color=f\"C{i}\")\n",
    "\n",
    "    # Définir les mêmes limites globales pour tous les subplots\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "\n",
    "    ax.set_xlabel(\"Principal Component 1\")\n",
    "    ax.set_ylabel(\"Principal Component 2\")\n",
    "    ax.set_title(f\"PCA Scatter Plot for {class_names[i]}\")\n",
    "    ax.legend()\n",
    "\n",
    "# Supprimer les axes inutilisés si le nombre de classes est inférieur au nombre de subplots\n",
    "for j in range(len(class_names), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Ajuster l'espace entre les subplots pour éviter le chevauchement\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Interprétation des résultats\n",
    "\n",
    "Ce graphique divise l'espace PCA en différentes sous-graphiques pour analyser la distribution individuelle de chaque catégorie.\n",
    "\n",
    "Peinture (Painting) : Distribuée assez largement, avec des valeurs de PCA couvrant une large plage. Cela suggère une diversité importante dans les caractéristiques des peintures, rendant difficile une séparation nette.\n",
    "\n",
    "Photo : Également bien dispersée, mais plus concentrée autour d'un noyau central, indiquant que les photos partagent plus de caractéristiques communes que les peintures.\n",
    "\n",
    "Schéma (Schematics) : La distribution est plus linéaire, suggérant que les caractéristiques des schémas sont fortement corrélées à une dimension spécifique de la PCA.\n",
    "\n",
    "Croquis (Sketch) : Cette catégorie semble former un cluster compact, mais décalé par rapport aux autres types d'images, suggérant qu'elle est relativement distincte dans l'espace des caractéristiques.\n",
    "\n",
    "Texte : Très fortement regroupé dans une zone précise, ce qui confirme que les images de texte ont des caractéristiques très spécifiques et homogènes."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Analyse TSNE\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Réduction dimensionnelle avec t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=123)\n",
    "tsne_result = tsne.fit_transform(images_reshaped)\n",
    "\n",
    "# Affichage t-SNE\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(len(np.unique(labels_sampled))):\n",
    "    plt.scatter(tsne_result[labels_sampled == i, 0], tsne_result[labels_sampled == i, 1], label=f\"{class_names[i]}\",\n",
    "                alpha=0.4)\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.title(\"2D t-SNE Visualization of Image Data\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Déterminer les limites globales des axes pour t-SNE\n",
    "x_min, x_max = tsne_result[:, 0].min(), tsne_result[:, 0].max()\n",
    "y_min, y_max = tsne_result[:, 1].min(), tsne_result[:, 1].max()\n",
    "\n",
    "# Créer des subplots avec 2 lignes et 3 colonnes (tu peux ajuster en fonction du nombre de classes)\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n",
    "axes = axes.flatten()  # Aplatir pour faciliter l'accès aux axes\n",
    "\n",
    "# Nombre de classes\n",
    "num_classes = len(np.unique(labels_sampled))\n",
    "\n",
    "# Affichage de t-SNE pour chaque classe dans un subplot distinct\n",
    "for i in range(num_classes):\n",
    "    ax = axes[i]  # Sélectionner l'axe pour la classe i\n",
    "\n",
    "    # Filtrer les points de la classe i\n",
    "    ax.scatter(tsne_result[labels_sampled == i, 0], tsne_result[labels_sampled == i, 1],\n",
    "               alpha=0.6, label=f\"{class_names[i]}\", color=f\"C{i}\")\n",
    "\n",
    "    # Définir les mêmes limites globales pour tous les subplots\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "\n",
    "    ax.set_xlabel(\"t-SNE Component 1\")\n",
    "    ax.set_ylabel(\"t-SNE Component 2\")\n",
    "    ax.set_title(f\"t-SNE for {class_names[i]}\")\n",
    "    ax.legend()\n",
    "\n",
    "# Supprimer les axes inutilisés si le nombre de classes est inférieur au nombre de subplots\n",
    "for j in range(num_classes, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Ajuster l'espace entre les subplots pour éviter le chevauchement\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Interprétation des résultats\n",
    "\n",
    "PrenomNom — 14:09\n",
    "Painting (bleu) et Photo (orange) : Très imbriquées, ce qui suggère que leurs caractéristiques sont proches dans l’espace des features. Cela peut s'expliquer par une similarité visuelle ou une proximité stylistique entre certains tableaux et photos réalistes.\n",
    "\n",
    "Schematics (vert clair) : Bien qu’elles s’étendent sur une large zone, elles forment une structure allongée partiellement chevauchée avec les clusters Painting/Photo. Cela indique une diversité interne importante dans cette classe, mais aussi quelques similarités avec des photos/peintures (probablement des schémas techniques complexes).\n",
    "\n",
    "Sketch (rouge) : Très bien séparées, formant un cluster compact et isolé en haut de la carte. Cela montre que les croquis possèdent des caractéristiques visuelles très distinctes (formes simples, contours marqués, peu de textures).\n",
    "\n",
    "Text (violet) : Regroupé de manière très dense et distincte à droite, formant plusieurs sous-clusters. Cela peut indiquer différents styles de textes (manuscrits, imprimés, tapés), mais reste bien séparé des autres types d’images, ce qui est cohérent avec la nature très différente du contenu textuel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation pour l'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "# Cette commande n'est pas nécessaire sur colab\n",
    "train_set = train_set.shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_set = val_set.prefetch(buffer_size=AUTOTUNE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la fonction de perte"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "total = sum(class_counts.values())\n",
    "weights = {class_name: (total / class_count) for class_name, class_count in class_counts.items()}\n",
    "\n",
    "max_weight = max(weights.values())\n",
    "class_weights = {class_name: weight / max_weight for class_name, weight in weights.items()}\n",
    "\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "# Appliquer les poids dans la fonction de perte\n",
    "class_weight_tensor = tf.constant([class_weights[class_name] for class_name in categories], dtype=tf.float32)\n",
    "\n",
    "print(\"Class_weight_tensor:\", class_weight_tensor)\n",
    "\n",
    "\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    weights = tf.gather(class_weight_tensor, tf.cast(y_true, tf.int32))\n",
    "    unweighted_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    return unweighted_loss * weights"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition du modèle"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = tf.keras.Sequential(layers=[\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Dropout(0.1, seed=42),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Dropout(0.1, seed=42),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Dropout(0.1, seed=42),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3, seed=42),\n",
    "    tf.keras.layers.Dense(units=num_classes, activation='softmax'), ]\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam', loss=weighted_loss, metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "visualkeras.layered_view(model, legend=True)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Définition des callbacks"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='./L1_model.keras', save_best_only=True)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "epochs = 10\n",
    "\n",
    "history = model.fit(train_set,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=val_set,\n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stopping, model_checkpoint, tensorboard_callback])\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%tensorboard --logdir logs/fit"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice de confusion et rapport de classification"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def confusion_matrix_and_report(model, val_set):\n",
    "    val_predictions = model.predict(val_set)\n",
    "    val_pred_labels = np.argmax(val_predictions, axis=1)\n",
    "\n",
    "    val_true_labels = np.concatenate([y for x, y in val_set], axis=0)\n",
    "\n",
    "    conf_matrix = confusion_matrix(val_true_labels, val_pred_labels)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(\"Matrice de Confusion\")\n",
    "    plt.xlabel(\"Classe Prédite\")\n",
    "    plt.ylabel(\"Classe Réelle\")\n",
    "    plt.show()\n",
    "    report = classification_report(val_true_labels, val_pred_labels, target_names=class_names)\n",
    "\n",
    "    print(\"Rapport de Classification :\")\n",
    "    print(report)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "confusion_matrix_and_report(model, val_set)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
