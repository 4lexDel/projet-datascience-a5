{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 1 - groupe 1\n",
    "\n",
    "Nous sommes le groupe 1 composé de :  \n",
    "- DELORME Alexandre\n",
    "- ENCOGNERE Yanis\n",
    "- MENNERON Laurine\n",
    "- PEREON Alexandre\n",
    "- ROCHARD Léo\n",
    "\n",
    "## Contenu du livrable\n",
    "TODO..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# TODO\n",
    "\n",
    "##  Analyse des Résultats et Biais/VarianceAnalyse\n",
    "\n",
    "Discussion sur le surapprentissage/sous-apprentissage\n",
    "\n",
    "Calcul et affichage d'une matrice de confusion\n",
    "\n",
    "Calcul de métriques supplémentaires (précision, rappel, F1-score)\n",
    "\n",
    "## À implémenter/discuter:\n",
    "\n",
    "Régularisation L2 dans les couches denses/convolutionnelles\n",
    "\n",
    "Ajustement du taux de dropout\n",
    "\n",
    "Utilisation de Batch Normalization\n",
    "\n",
    "Transfer learning avec un modèle pré-entraîné (VGG16, ResNet, etc.)\n",
    "\n",
    "Grid search pour optimiser les hyperparamètres\n",
    "\n",
    "## Documentation et Justifications\n",
    "\n",
    "Schéma de l'architecture du réseau (peut être généré avec tf.keras.utils.plot_model)\n",
    "\n",
    "Justification des choix (architecture, hyperparamètres, etc.)\n",
    "\n",
    "Analyse des résultats obtenus\n",
    "\n",
    "Discussion sur les difficultés rencontrées (images réalistes dans les peintures)\n",
    "\n",
    "## Gestion des Données Déséquilibrées\n",
    "\n",
    "Analyse du déséquilibre entre classes photo/non-photo\n",
    "\n",
    "Techniques pour gérer le déséquilibre (poids de classe, oversampling, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "TODO...\n",
    "- Chargement du dataset\n",
    "- Visualisation\n",
    "- Répartition des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%load_ext tensorboard\n",
    "import imghdr\n",
    "import os\n",
    "import pathlib\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Chargement du dataset\n"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ID du fichier (extrait de l'URL)\n",
    "file_id = \"1Q5QttYirEHvto38l6e6uk66Fjvuk_V7I\"\n",
    "dataset_path = \"dataset_livrable_1\"\n",
    "zip_path = dataset_path + \".zip\"\n",
    "extract_dir = pathlib.Path(zip_path).parent / dataset_path\n",
    "\n",
    "if not os.path.exists(extract_dir):\n",
    "    print(f\"Le dossier '{extract_dir}' n'existe pas. Téléchargement en cours...\")\n",
    "    gdown.download(f\"https://drive.google.com/uc?id={file_id}\", zip_path, quiet=False)\n",
    "\n",
    "    print(f\"Extraction ZIP en cours...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\"Extraction Zip terminée\")\n",
    "else:\n",
    "    print(f\"Le dossier '{extract_dir}' existe déjà. Téléchargement et extraction non nécessaires.\")\n",
    "\n",
    "data_dir = extract_dir\n",
    "print(f\"Dataset disponible dans : {data_dir}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "categories = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "print(f\"Catégories détectées : {categories}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré traitement des images"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Function to detect and remove corrupted files\n",
    "num_skipped = 0\n",
    "for folder_name in categories:\n",
    "    folder_path = data_dir / folder_name\n",
    "    for fname in os.listdir(folder_path):\n",
    "        fpath = folder_path / fname\n",
    "        try:\n",
    "            # Check if the file is a valid image\n",
    "            if imghdr.what(fpath) is None:\n",
    "                raise IOError(f\"File is not a valid image: {fpath}\")\n",
    "\n",
    "            # Try to open the image file\n",
    "            with tf.io.gfile.GFile(fpath, 'rb') as f:\n",
    "                img_content = f.read()\n",
    "            try:\n",
    "                img = tf.io.decode_image(img_content, dtype=tf.dtypes.uint8, channels=3)\n",
    "            except tf.errors.InvalidArgumentError as e:\n",
    "                print(f\"Invalid image detected and removed: {fpath}\")\n",
    "                num_skipped += 1\n",
    "                os.remove(fpath)\n",
    "                continue\n",
    "        except (IOError, SyntaxError) as e:\n",
    "            print(f\"Corrupted file detected and removed: {fpath}\")\n",
    "            num_skipped += 1\n",
    "            os.remove(fpath)\n",
    "\n",
    "print(f\"Number of corrupted files removed: {num_skipped}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des images"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "validation_split = 0.2\n",
    "seed = 42\n",
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "train_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=validation_split,\n",
    "    subset=\"training\",\n",
    "    seed=seed,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_names=categories)\n",
    "\n",
    "val_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=validation_split,\n",
    "    subset=\"validation\",\n",
    "    seed=seed,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_names=categories)\n",
    "\n",
    "class_names = train_set.class_names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"Classes found: {class_names}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Visualisation des différentes classes"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "for images, labels in train_set.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Répartition des classes au sein du dataset"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def count_labels(dataset):\n",
    "    label_counts = {}\n",
    "    for _, labels in dataset:\n",
    "        for label in labels.numpy():\n",
    "            class_name = class_names[label]\n",
    "            label_counts[class_name] = label_counts.get(class_name, 0) + 1\n",
    "    return label_counts\n",
    "\n",
    "\n",
    "# Compter les labels dans les jeux d'entraînement et de validation\n",
    "train_label_counts = count_labels(train_set)\n",
    "val_label_counts = count_labels(val_set)\n",
    "\n",
    "# Extraire les données pour la visualisation\n",
    "labels, train_counts = zip(*train_label_counts.items())\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Répartition des labels dans le jeu d'entraînement :\")\n",
    "print(train_label_counts)\n",
    "\n",
    "# Affichage de la répartition\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(labels, train_counts, color=\"skyblue\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Répartition des labels - Jeu d\\'entraînement')\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Nombre d\\'images')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation pour l'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# Utiliser que 1/10 des données pour l'entrainement\n",
    "train_size = int(len(train_set) // 10)\n",
    "train_set_small = train_set.take(train_size)\n",
    "train_set_small = train_set_small.shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Valider avec votre jeu de validation\n",
    "val_set = val_set.prefetch(buffer_size=AUTOTUNE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition du modèle"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "model = tf.keras.Sequential(layers=[\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Dropout(0.1, seed=42),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Dropout(0.1, seed=42),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Dropout(0.1, seed=42),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3, seed=42),\n",
    "    tf.keras.layers.Dense(units=num_classes, activation='softmax'), ]\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "#### Définition des callbacks"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='./L1_model.keras', save_best_only=True)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "epochs = 20\n",
    "\n",
    "history = model.fit(train_set,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=val_set,\n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "%tensorboard --logdir logs/fit",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Matrice de confusion"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "val_predictions = model.predict(val_set)\n",
    "val_pred_labels = np.argmax(val_predictions, axis=1)\n",
    "\n",
    "val_true_labels = np.concatenate([y for x, y in val_set], axis=0)\n",
    "\n",
    "conf_matrix = confusion_matrix(val_true_labels, val_pred_labels)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(\"Matrice de Confusion\")\n",
    "plt.xlabel(\"Classe Prédite\")\n",
    "plt.ylabel(\"Classe Réelle\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
