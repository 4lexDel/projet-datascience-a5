{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Livrable 1 - groupe 1\n",
    "\n",
    "Nous sommes le groupe 1 composé de :  \n",
    "- DELORME Alexandre\n",
    "- ENCOGNERE Yanis\n",
    "- MENNERON Laurine\n",
    "- PEREON Alexandre\n",
    "- ROCHARD Léo\n",
    "\n",
    "## Contenu du livrable\n",
    "Dans ce premier livrable, nous allons nous intéresser à la classification multiclasses de nos données. \n",
    "Nous avons à disposition : des peintures, des schémas/graphes, des portraits dessinés en noir et blanc, des images de textes scannés et des photos.  \n",
    "Nous réaliserons dans un premier temps une analyse exploratoire des données (`EDA`) afin de mieux comprendre les caractéristiques de notre jeu de données, d’identifier d’éventuelles anomalies ou déséquilibres, et de préparer les données pour l’entraînement. Cette étape inclura notamment une visualisation des classes, la vérification des tailles et des formats d’images, ainsi que des traitements éventuels (redimensionnement, normalisation, data augmentation).  \n",
    "Nous allons ensuite définir `l’architecture de notre modèle de classification d’images`. Nous justifierons les choix réalisés concernant la structure de notre réseau ainsi que les choix des `hyperparamètres` de nos différentes couches (nombre de filtres, taille des noyaux de convolution, fonctions d’activation, couches de pooling, etc.) et les paramètres d'entraînement (learning rate, batch size, nombre d’époques, fonction de perte, optimiseur, etc.).  \n",
    "Nous entraînerons notre modèle en suivant une démarche itérative : \n",
    "\n",
    "__Entraînement :__  \n",
    "Nous procéderons à l’entraînement du modèle sur notre jeu de données d’apprentissage en intégrant des `techniques de régularisation` (dropout, batch normalization, augmentation de données) pour limiter le sur-apprentissage. \n",
    "\n",
    "__Benchmark :__  \n",
    "Nous mettrons en place un benchmark afin de comparer les performances de différentes architectures ou configurations de notre modèle. Nous testerons plusieurs variantes pour évaluer les impacts sur les performances. \n",
    "\n",
    "__Métriques :__  \n",
    "Les performances du modèle seront évaluées à l’aide de plusieurs `métriques` : accuracy, matrice de confusion, précision, rappel et F1-score. Nous suivrons également la progression de la courbe d’apprentissage pour identifier les cas de sur et de sous-apprentissage. \n",
    "\n",
    "__Conclusion :__  \n",
    "Enfin, nous conclurons ce livrable en récapitulant les résultats obtenus, les points forts et les limites de notre approche actuelle. Nous proposerons également des pistes d’amélioration possibles que nous pourrons appliquer pour les prochains livrables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des bibliothèques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici les bibliothèques principales que nous avons utilisées pour ce projet :  \n",
    "- __NumPy :__ facilite et améliore la manipulation et le `traitement des données`\n",
    "- __TensorFlow :__ bibliothèque principale de Deep Learning utilisée pour construire, entraîner et évaluer nos modèles de `classification`. Elle fournit des structures de données adaptées, des couches prédéfinies ainsi que les outils nécessaires pour la création de `réseaux de neurones` et leur entraînement\n",
    "- __VisualKeras :__ outil permettant de générer des représentations graphiques de l’architecture de nos modèles Keras, facilitant ainsi leur visualisation et leur compréhension\n",
    "- __Scikit-learn (sklearn) :__ utiliser pour `visualiser` et évaluer notre modèle. Utile également pour la réalisation de l’analyse `ACP` et `TSME`\n",
    "- __Matplotlib et Seaborn :__ utilisation pour la visualisation de nos différents résultats sous forme de `graphiques`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:12:18.163766Z",
     "start_time": "2025-04-09T13:12:13.498600Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "import os\n",
    "import pathlib\n",
    "import zipfile\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "import gdown\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import visualkeras\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sélection de la source de données\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette partie permet de récuperer le dataset depuis le drive afin d'automatiser la `pipeline`.  \n",
    "L'objectif est d'utiliser un dataset commun au sein du groupe. \n",
    "\n",
    "Ce dernier à le dossier `Sketch` complété par des données trouvés dans les sources suivantes : [croquis](https://paperswithcode.com/dataset/sketch) || [visages réalistes](https://www.kaggle.com/datasets/arbazkhan971/cuhk-face-sketch-database-cufs/data)\n",
    "\n",
    "Ce qui fait passer le dossier sketch de `606 visages` et `800 croquis` avec un ration de `43%`/`57%` entre visages/croquis à `1200 visages` et `3200 croquis` pour un ratio final équivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:12:18.190364Z",
     "start_time": "2025-04-09T13:12:18.165644Z"
    }
   },
   "outputs": [],
   "source": [
    "# ID du fichier (extrait de l'URL)\n",
    "file_id = \"1d0k3mXd93JM0fLLYrUZRr4Un1F_TnuHi\"\n",
    "dataset_path = \"dataset_livrable_1\"\n",
    "zip_path = dataset_path + \".zip\"\n",
    "extract_dir = pathlib.Path(zip_path).parent / dataset_path\n",
    "reduce_dataset = True\n",
    "\n",
    "if not os.path.exists(extract_dir):\n",
    "    print(f\"Le dossier '{extract_dir}' n'existe pas. Téléchargement en cours...\")\n",
    "    gdown.download(f\"https://drive.google.com/uc?id={file_id}\", zip_path, quiet=False)\n",
    "\n",
    "    print(f\"Extraction ZIP en cours...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "    print(f\"Extraction Zip terminée\")\n",
    "else:\n",
    "    print(f\"Le dossier '{extract_dir}' existe déjà. Téléchargement et extraction non nécessaires.\")\n",
    "\n",
    "data_dir = extract_dir\n",
    "print(f\"Dataset disponible dans : {data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Détection des catégories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici ci-contre la liste des classes de notre dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:12:21.394648Z",
     "start_time": "2025-04-09T13:12:21.382535Z"
    }
   },
   "outputs": [],
   "source": [
    "categories = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "print(f\"Catégories détectées : {categories}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré traitement des images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certaines images de notre dataset sont `corrompues` ou ne sont pas au bon format (exemple avec un fichier « .ini »). Nous devons donc nous assurer de filtrer ces données avant de créer notre dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppresion des fichiers corrompus ou non images\n",
    "def clean_images_dataset(dataset_path_arg):\n",
    "    \"\"\"\n",
    "    Fonction pour nettoyer le dataset en supprimant les fichiers corrompus ou non images.\n",
    "    \"\"\"\n",
    "    # Dictionnaire pour stocker le nombre d'images corrompues par classe\n",
    "    corrupted_count_by_class = defaultdict(int)\n",
    "    dataset_path = dataset_path_arg\n",
    "    print(\"Début de la vérification des images ...\")\n",
    "\n",
    "    # Récupération de toutes les images pour calculer la progression\n",
    "    all_files = []\n",
    "    for dir_name in os.listdir(dataset_path):\n",
    "        dir_path = os.path.join(dataset_path, dir_name)\n",
    "        for file_name in os.listdir(dir_path):\n",
    "            all_files.append((dir_name, dir_path, file_name))\n",
    "\n",
    "    total_files = len(all_files)\n",
    "    checked_files = 0  # Pour la progression\n",
    "\n",
    "    # Parcours des images avec affichage de la progression\n",
    "    for dir_name, dir_path, file_name in all_files:\n",
    "        if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "            try:\n",
    "                with open(os.path.join(dir_path, file_name), 'rb') as file:\n",
    "                    img_bytes = file.read()  # Lire les bytes de l'image\n",
    "                    img = tf.image.decode_image(img_bytes)  # Essayer de décoder l'image\n",
    "            except Exception as e:\n",
    "                corrupted_count_by_class[dir_name] += 1\n",
    "                print(f\"\\nImage corrompue : {file_name} dans {dir_name}. Exception: {e}\")\n",
    "                os.remove(os.path.join(dir_path, file_name))\n",
    "                print(f\"Image {file_name} supprimée.\")\n",
    "        else:\n",
    "            corrupted_count_by_class[dir_name] += 1\n",
    "            print(f\"\\nLe fichier {file_name} dans {dir_name} n'est pas une image.\")\n",
    "            os.remove(os.path.join(dir_path, file_name))\n",
    "            print(f\"Fichier {file_name} supprimé.\")\n",
    "\n",
    "        # Mise à jour de la progression\n",
    "        checked_files += 1\n",
    "        progress = (checked_files / total_files) * 100\n",
    "        print(f\"\\rProgression : [{int(progress)}%] {checked_files}/{total_files} images vérifiées\", end=\"\")\n",
    "\n",
    "    print(\"\\nVérification des fichiers terminée.\")\n",
    "\n",
    "    # Affichage du nombre d'images corrompues par dossier\n",
    "    for dir_name, count in corrupted_count_by_class.items():\n",
    "        print(f\"Dossier {dir_name} : {count} images corrompues\")\n",
    "\n",
    "    # Nombre total d'images corrompues\n",
    "    total_corrupted = sum(corrupted_count_by_class.values())\n",
    "    print(f\"Nombre total d'images corrompues ou non image : {total_corrupted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul du nombre de données par classe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous comptons ensuite le nombre d’images par dossier pour pouvoir déterminer la `répartition des données par classe`.  \n",
    "Nous observons que la classe Sketch est très largement `sous représentés` avec seulement `4394` données contre une moyenne de `9998` pour les autres classes.  \n",
    "Pour pouvoir compenser ce déséquilibre, nous comptons sur la `loss pondération`. Ainsi, notre modèle se trouvera d’autant plus pénalisé si il effectue une mauvaise prédiction sur une image de classe Sketch.  \n",
    "Nous avons également pensé à générer de nouvelles données à partir de `modèles génératifs` tel qu’un `GAN` et un `auto-encodeur de type VAE`. Cependant, les résultats que nous avons obtenus n’étant pas suffisamment satisfaisants, nous avons décidé de garder uniquement la loss pondération. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:12:24.631216Z",
     "start_time": "2025-04-09T13:12:24.336141Z"
    }
   },
   "outputs": [],
   "source": [
    "class_counts = {class_name: len(list((data_dir / class_name).glob('*'))) for class_name in categories}\n",
    "class_counts['Sketch'] = len(list((data_dir / 'Sketch' / 'jpg').glob('*'))) + len(\n",
    "    list((data_dir / 'Sketch' / 'png').glob('*')))\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons finalement créer notre dataset à partir des images que nous avons traités précédemment. Nous avons défini un `batch de 32`, un batch élevé nous permet de gagner en performance lors de l'entraînement  au détriment de la mémoire, cette valeur est un bon compromis. Nous avons séparé nos données en 2 ensembles : `80%` pour l’entraînement et `20%` pour la validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:12:55.760886Z",
     "start_time": "2025-04-09T13:12:49.935311Z"
    }
   },
   "outputs": [],
   "source": [
    "validation_split = 0.2\n",
    "seed = 42\n",
    "batch_size = 32\n",
    "img_height = 180\n",
    "img_width = 180\n",
    "\n",
    "train_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=validation_split,\n",
    "    subset=\"training\",\n",
    "    seed=seed,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_names=categories)\n",
    "\n",
    "val_set = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_dir,\n",
    "    validation_split=validation_split,\n",
    "    subset=\"validation\",\n",
    "    seed=seed,\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_names=categories)\n",
    "\n",
    "class_names = train_set.class_names\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"Classes found: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des différentes classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un échantillon d'images de notre dataset pour les différentes classes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "for images, labels in train_set.take(1):\n",
    "    for i in range(9):\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "        plt.title(class_names[labels[i]])\n",
    "        plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Répartition des classes au sein du dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le graphique ci-dessous représente la `répartition des données` pour nos datasets `d'entraînement` et de `validation` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compter les échantillons par classe dans train et val\n",
    "def count_per_class(dataset, class_names):\n",
    "    class_counts = defaultdict(int)\n",
    "    for images, labels in dataset:\n",
    "        for label in labels.numpy():\n",
    "            class_counts[class_names[label]] += 1\n",
    "    return class_counts\n",
    "\n",
    "train_label_counts = count_per_class(train_set, class_names)\n",
    "val_label_counts = count_per_class(val_set, class_names)\n",
    "\n",
    "# S'assurer que toutes les classes sont présentes dans les deux sets\n",
    "classes = class_names\n",
    "train_counts = [train_label_counts.get(cls, 0) for cls in classes]\n",
    "val_counts = [val_label_counts.get(cls, 0) for cls in classes]\n",
    "\n",
    "x = np.arange(len(classes))  # position des classes\n",
    "\n",
    "# Création du barplot\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_width = 0.35\n",
    "plt.bar(x - bar_width/2, train_counts, width=bar_width, label='Train Set', color='skyblue')\n",
    "plt.bar(x + bar_width/2, val_counts, width=bar_width, label='Validation Set', color='orange')\n",
    "\n",
    "# Mise en forme\n",
    "plt.xticks(x, classes, rotation=45)\n",
    "plt.xlabel(\"Classes\")\n",
    "plt.ylabel(\"Nombre d'images\")\n",
    "plt.title(\"Répartition des images par classe (Train vs Validation)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Préparation pour analyse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir les images et labels en arrays\n",
    "images, labels = zip(*[(image_batch.numpy(), label_batch.numpy()) for image_batch, label_batch in train_set])\n",
    "images = np.concatenate(images)\n",
    "labels = np.concatenate(labels)\n",
    "\n",
    "# Sélectionner 20% des données aléatoirement\n",
    "num_samples = images.shape[0]\n",
    "sample_size = int(num_samples * 0.20)  # 20% du dataset\n",
    "indices = np.random.choice(num_samples, sample_size, replace=False)\n",
    "\n",
    "# Extraire les échantillons sélectionnés\n",
    "images_sampled = images[indices]\n",
    "labels_sampled = labels[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse en composantes principales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réduction dimensionnelle avec PCA\n",
    "images_reshaped = images_sampled.reshape(images_sampled.shape[0], -1)\n",
    "pca = PCA(n_components=0.95)\n",
    "pca_result = pca.fit_transform(images_reshaped)\n",
    "\n",
    "# Affichage PCA\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(len(class_names)):\n",
    "    plt.scatter(pca_result[labels_sampled == i, 0], pca_result[labels_sampled == i, 1], label=class_names[i], alpha=0.4)\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.title(\"2D PCA Plot of Image Data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_min = np.round(pca_result[:, 0].min(), -4)\n",
    "x_max = np.round(pca_result[:, 0].max(), -4)\n",
    "y_min = np.round(pca_result[:, 1].min(), -4)\n",
    "y_max = np.round(pca_result[:, 1].max(), -4)\n",
    "\n",
    "# Créer des subplots avec 2 lignes et 3 colonnes (ajuster en fonction du nombre de classes)\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n",
    "axes = axes.flatten()  # Aplatir la matrice d'axes pour un accès facile\n",
    "\n",
    "# Affichage de PCA pour chaque classe dans un subplot distinct\n",
    "for i in range(len(class_names)):\n",
    "    ax = axes[i]  # Sélectionner l'axe pour la classe i\n",
    "\n",
    "    # Filtrer les points de la classe i\n",
    "    ax.scatter(pca_result[labels_sampled == i, 0], pca_result[labels_sampled == i, 1],\n",
    "               alpha=0.6, label=f\"{class_names[i]}\", color=f\"C{i}\")\n",
    "\n",
    "    # Définir les mêmes limites globales pour tous les subplots\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "\n",
    "    ax.set_xlabel(\"Principal Component 1\")\n",
    "    ax.set_ylabel(\"Principal Component 2\")\n",
    "    ax.set_title(f\"PCA Scatter Plot for {class_names[i]}\")\n",
    "    ax.legend()\n",
    "\n",
    "# Supprimer les axes inutilisés si le nombre de classes est inférieur au nombre de subplots\n",
    "for j in range(len(class_names), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Ajuster l'espace entre les subplots pour éviter le chevauchement\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interprétation des résultats\n",
    "\n",
    "Ce graphique divise l'espace PCA en différentes sous-graphiques pour analyser la distribution individuelle de chaque catégorie.\n",
    "\n",
    "Peinture (Painting) : Distribuée assez largement, avec des valeurs de PCA couvrant une large plage. Cela suggère une diversité importante dans les caractéristiques des peintures, rendant difficile une séparation nette.\n",
    "\n",
    "Photo : Également bien dispersée, mais plus concentrée autour d'un noyau central, indiquant que les photos partagent plus de caractéristiques communes que les peintures.\n",
    "\n",
    "Schéma (Schematics) : La distribution est plus linéaire, suggérant que les caractéristiques des schémas sont fortement corrélées à une dimension spécifique de la PCA.\n",
    "\n",
    "Croquis (Sketch) : Cette catégorie semble former un cluster compact, mais décalé par rapport aux autres types d'images, suggérant qu'elle est relativement distincte dans l'espace des caractéristiques.\n",
    "\n",
    "Texte : Très fortement regroupé dans une zone précise, ce qui confirme que les images de texte ont des caractéristiques très spécifiques et homogènes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse TSNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Réduction dimensionnelle avec t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=123)\n",
    "tsne_result = tsne.fit_transform(images_reshaped)\n",
    "\n",
    "# Affichage t-SNE\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i in range(len(np.unique(labels_sampled))):\n",
    "    plt.scatter(tsne_result[labels_sampled == i, 0], tsne_result[labels_sampled == i, 1], label=f\"{class_names[i]}\",\n",
    "                alpha=0.4)\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "plt.title(\"2D t-SNE Visualization of Image Data\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Déterminer les limites globales des axes pour t-SNE\n",
    "x_min, x_max = tsne_result[:, 0].min(), tsne_result[:, 0].max()\n",
    "y_min, y_max = tsne_result[:, 1].min(), tsne_result[:, 1].max()\n",
    "\n",
    "# Créer des subplots avec 2 lignes et 3 colonnes (tu peux ajuster en fonction du nombre de classes)\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10))\n",
    "axes = axes.flatten()  # Aplatir pour faciliter l'accès aux axes\n",
    "\n",
    "# Nombre de classes\n",
    "num_classes = len(np.unique(labels_sampled))\n",
    "\n",
    "# Affichage de t-SNE pour chaque classe dans un subplot distinct\n",
    "for i in range(num_classes):\n",
    "    ax = axes[i]  # Sélectionner l'axe pour la classe i\n",
    "\n",
    "    # Filtrer les points de la classe i\n",
    "    ax.scatter(tsne_result[labels_sampled == i, 0], tsne_result[labels_sampled == i, 1],\n",
    "               alpha=0.6, label=f\"{class_names[i]}\", color=f\"C{i}\")\n",
    "\n",
    "    # Définir les mêmes limites globales pour tous les subplots\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "\n",
    "    ax.set_xlabel(\"t-SNE Component 1\")\n",
    "    ax.set_ylabel(\"t-SNE Component 2\")\n",
    "    ax.set_title(f\"t-SNE for {class_names[i]}\")\n",
    "    ax.legend()\n",
    "\n",
    "# Supprimer les axes inutilisés si le nombre de classes est inférieur au nombre de subplots\n",
    "for j in range(num_classes, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Ajuster l'espace entre les subplots pour éviter le chevauchement\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interprétation des résultats\n",
    "Painting (bleu) et Photo (orange) : Très imbriquées, ce qui suggère que leurs caractéristiques sont proches dans l’espace des features. Cela peut s'expliquer par une similarité visuelle ou une proximité stylistique entre certains tableaux et photos réalistes.\n",
    "\n",
    "Schematics (vert clair) : Bien qu’elles s’étendent sur une large zone, elles forment une structure allongée partiellement chevauchée avec les clusters Painting/Photo. Cela indique une diversité interne importante dans cette classe, mais aussi quelques similarités avec des photos/peintures (probablement des schémas techniques complexes).\n",
    "\n",
    "Sketch (rouge) : Très bien séparées, formant un cluster compact et isolé en haut de la carte. Cela montre que les croquis possèdent des caractéristiques visuelles très distinctes (formes simples, contours marqués, peu de textures).\n",
    "\n",
    "Text (violet) : Regroupé de manière très dense et distincte à droite, formant plusieurs sous-clusters. Cela peut indiquer différents styles de textes (manuscrits, imprimés, tapés), mais reste bien séparé des autres types d’images, ce qui est cohérent avec la nature très différente du contenu textuel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation pour l'entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans cette partie, vous devrez utiliser les fonctions [`Dataset.cache`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#cache) et [`Dataset.prefetch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#prefetch) afin de configurer les données pour améliorer les performances de la façon suivante :\n",
    "- `Dataset.cache()` : cette fonction sert à forcer le maintien des données en cache dans la mémoire. Vu que le réseau de neurones fait plusieurs passes (qu'on nomme _époque_ ou _epoch_ en anglais) sur les données durant l'apprentissage, cette fonction permet de ne pas avoir à recharger les images à chaque fois. \n",
    "- `Dataset.prefetch()` : cette fonction permet de faire le prétraitement de l'élément courant du jeu de données (par exemple le batch suivant) en même temps que l'entraînement/évaluation du batch courant par le modèle. Dans un environnement multi-processeurs ou multi-cœur, c'est un gain de temps non-négligeable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:13:05.967919Z",
     "start_time": "2025-04-09T13:13:05.952021Z"
    }
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "# Cette commande n'est pas nécessaire sur colab\n",
    "train_set = train_set.shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_set = val_set.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition de la fonction de perte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons une `fonction de perte` adaptée à notre problème de classification multiclasses. Cette fonction permet de calculer l’erreur entre les prédictions du modèle et les valeurs réelles afin d’`ajuster les poids` du réseau lors de l’entraînement. \n",
    "Afin de limiter l’impact du déséquilibre des classes dans nos données, nous avons défini des poids spécifiques pour chaque classe (`class weights`), attribuant un poids plus élevé aux classes sous-représentées afin de `compenser` leur faible occurrence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:13:08.223445Z",
     "start_time": "2025-04-09T13:13:08.212377Z"
    }
   },
   "outputs": [],
   "source": [
    "total = sum(class_counts.values())\n",
    "weights = {class_name: (total / class_count) for class_name, class_count in class_counts.items()}\n",
    "\n",
    "max_weight = max(weights.values())\n",
    "class_weights = {class_name: weight / max_weight for class_name, weight in weights.items()}\n",
    "\n",
    "print(\"Class Weights:\", class_weights)\n",
    "\n",
    "# Appliquer les poids dans la fonction de perte\n",
    "class_weight_tensor = tf.constant([class_weights[class_name] for class_name in categories], dtype=tf.float32)\n",
    "\n",
    "print(\"Class_weight_tensor:\", class_weight_tensor)\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    weights = tf.gather(class_weight_tensor, tf.cast(y_true, tf.int32))\n",
    "    unweighted_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)\n",
    "    return unweighted_loss * weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "Nous avons construit un `réseau de neurones convolutionnels` (CNN), architecture particulièrement adaptée au traitement et à la `classification d’images`.  \n",
    "Notre modèle suit une structure classique composée de plusieurs `couches` successives :  \n",
    "- Couches de convolution : extraction des caractéristiques locales des images à l’aide de `filtres convolutionnels`\n",
    "- Couches de pooling : `réduction de la dimensionnalité` et des paramètres, limitation du risque de sur-apprentissage\n",
    "- Couches finales : `flatten pour vectoriser les données`, suivi de couches Dense pour la prise de décision. La dernière\n",
    "- Couche Dense : utilise une `activation softmax` adaptée à la classification multiclasse\n",
    "\n",
    "#### Techniques complémentaires\n",
    "- Application de `Dropout` après certaines couches pour `limiter le sur-apprentissage` (overfitting) en désactivant aléatoirement des neurones lors de l’entraînement\n",
    "- Choix du `nombre de couches`, du `nombre de neurones` et des `hyperparamètres` (kernel size, filtres, taux de dropout, etc.) basé sur :  \n",
    "    * Nos recherches personnelles et expérimentations (tests itératifs, résultats obtenus en WKS)\n",
    "    * Des comparaisons avec l’état de l’art sur des projets similaires\n",
    "    * L’analyse des performances obtenues lors de nos différents essais (benchmark et validation croisée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:17:11.740153Z",
     "start_time": "2025-04-09T13:17:11.588583Z"
    }
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential(layers=[\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu', input_shape=(img_height, img_width, 3)),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Dropout(0.1, seed=42),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Dropout(0.1, seed=42),\n",
    "\n",
    "    tf.keras.layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Dropout(0.1, seed=42),\n",
    "\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3, seed=42),\n",
    "    tf.keras.layers.Dense(units=num_classes, activation='softmax'), ]\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam', loss=weighted_loss, metrics=['accuracy'])\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici ci-contre la visualisation de notre modèle, cette dernière nous permet de représenter de manière claire l’architecture de notre réseau de neurones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualkeras.layered_view(model, legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrainement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous pouvons finalement entraîner notre modèle.  \n",
    "Nous définissons des `callbacks` :  \n",
    "- Early stopping pour arrêter l’entraînement quand le modèle n’apprend plus (ou sur-apprend)\n",
    "- Création de checkpoint pour sauvegarder notre modèle\n",
    "- Nous avons défini les paramètres nécessaires : nombre d’epochs, dataset, callback, batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Définition des callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:17:15.279802Z",
     "start_time": "2025-04-09T13:17:15.263867Z"
    }
   },
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='./L1_model.keras', save_best_only=True)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:51:32.806206Z",
     "start_time": "2025-04-09T13:17:19.133409Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "history = model.fit(train_set,\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    validation_data=val_set,\n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stopping, model_checkpoint, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Courbe d’apprentissage\n",
    "\n",
    "o Interprétation de la courbe :\n",
    "\n",
    "- Surapprentissage : la situation de sur-apprentissage peut être détectée lorsque la courbe de loss de validation commence à augmenter par rapport à la loss d’entraînement qui est stabilisée. Pour lutter contre ce phénomène nous avons mis en place des couches de Dropout ainsi que la callback permettant le early-stopping. Ces actions préventives permette de d'arrêter l'entraînement avant de rentrer dans le cas échéant.\n",
    "\n",
    "- Sous apprentissage : le modèle n’a pas encore extrait toutes les caractéristiques pertinentes des données. Les performances varient encore beaucoup d’une epoch à l’autre. Il convient d’ajouter des données ou des epochs pour améliorer les performances du modèle.Dans notre cas, nous n'avons pas eu ce problème car le dataset est correctement répartit avec une proportion de 80% de données d'entrainements pour 20% de données de validations sur un nombre total avoisinant les 45 000 données au total. \n",
    "\n",
    "- Validation vs training set : Comme évoqué ci-dessus, nous avons divisé notre jeu de données en 2 parties : une dédiée à l’entraînement de notre modèle, et l’autre dédiée à la validation de son apprentissage. Ainsi nous vérifions sa capacité de généralisation sur des nouvelles données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(len(acc))\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T09:04:02.327703Z",
     "start_time": "2025-04-09T09:04:02.016987Z"
    }
   },
   "outputs": [],
   "source": [
    "model = load_model('L1_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métriques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice de confusion et rapport de classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:52:04.815273Z",
     "start_time": "2025-04-09T13:52:04.784740Z"
    }
   },
   "outputs": [],
   "source": [
    "def confusion_matrix_and_report(model, val_set, class_names):\n",
    "    print(\"Classes utilisées :\", class_names)\n",
    "\n",
    "    # Préparer les données de validation sans modification\n",
    "    val_set = val_set.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "    # Obtenir les vraies étiquettes et les prédictions\n",
    "    val_true_labels = []\n",
    "    val_pred_labels = []\n",
    "\n",
    "    for images, labels in val_set:\n",
    "        # Prédictions pour le batch\n",
    "        predictions = model.predict(images, verbose=0)\n",
    "        val_pred_labels.extend(np.argmax(predictions, axis=1))\n",
    "        val_true_labels.extend(labels.numpy())\n",
    "\n",
    "    # Convertir en arrays numpy\n",
    "    val_true_labels = np.array(val_true_labels)\n",
    "    val_pred_labels = np.array(val_pred_labels)\n",
    "\n",
    "    # Calculer la matrice de confusion\n",
    "    conf_matrix = confusion_matrix(val_true_labels, val_pred_labels)\n",
    "\n",
    "    # Afficher la matrice de confusion\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(\"Matrice de Confusion\")\n",
    "    plt.xlabel(\"Classe Prédite\")\n",
    "    plt.ylabel(\"Classe Réelle\")\n",
    "    plt.show()\n",
    "\n",
    "    # Afficher le rapport de classification\n",
    "    report = classification_report(val_true_labels, val_pred_labels, target_names=class_names)\n",
    "    print(\"Rapport de Classification :\")\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T13:52:41.613134Z",
     "start_time": "2025-04-09T13:52:07.320081Z"
    }
   },
   "outputs": [],
   "source": [
    "confusion_matrix_and_report(model, val_set, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrice de confusion\n",
    "\n",
    "La matrice de confusion permet de comparer la classe réelle et la classe prédite pour les données de validation envoyées au modèle. Cette matrice nous permet de mieux comprendre les erreurs de notre modèle afin de pouvoir s’adapter en conséquence. En l’occurrence, nous constatons que notre modèle a parfois du mal à distinguer les photos des peintures (qui peuvent avoir un rendu assez réaliste). Ainsi nous pouvons proposer des pistes d’amélioration telles que l’augmentation de la qualité de l’image entrante par exemple.\n",
    "\n",
    "Precision, Recall, F1score : Ces métriques permettent de mieux analyser les performances du modèle, et donc de s’assurer qu’il n’y a pas trop de confusions entre les différentes classes.\n",
    "\n",
    "### Résultats globaux\n",
    "- **Accuracy** : **85%**\n",
    "- **Macro Average** (moyenne non pondérée) :\n",
    "  - Précision : **0.87**\n",
    "  - Rappel : **0.87**\n",
    "  - F1-score : **0.86**\n",
    "- **Weighted Average** (pondérée par le nombre d'exemples) :\n",
    "  - Précision : **0.85**\n",
    "  - Rappel : **0.85**\n",
    "  - F1-score : **0.84**\n",
    "### Forces\n",
    "- **Sketch** et **Text** sont très bien reconnus, avec des F1-scores proches de 1.\n",
    "- **Schematics** présente un bon équilibre entre précision et rappel.\n",
    "### Faiblesses\n",
    "- **Photo** :\n",
    "  - Faible rappel (**0.62**) → le modèle rate de nombreux vrais exemples de \"Photo\".\n",
    "- **Painting** :\n",
    "  - Faible précision (**0.70**) → le modèle confond \"Painting\" avec des \"Photos\" et des \"Schematics\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO :\n",
    "- Rappel des objectives\n",
    "- Résultats obtenues\n",
    "\n",
    "OUVERTURE :\n",
    "- Data augmentation : la data augmentation permet de rendre notre modèle plus robuste \n",
    "- Utilisation de transfert learning avec un modèle pré-entraîné (VGG16, ResNet, etc.)  \n",
    "- Autoencoder livrable 2 => réduire la taille des données avec l’encodeur et utilisation du décodeur pour enlever le bruit\n",
    "- Modèle BIAIS dans les couches "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
