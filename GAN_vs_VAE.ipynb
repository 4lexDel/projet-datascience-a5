{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparaison des GAN et des VAE pour la génération d'images (sketch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importation des librairies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras import layers\n",
        "from keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bv-CLyZQnKf1",
        "outputId": "b9cef99c-67ea-487c-a18b-e935522756f0"
      },
      "outputs": [],
      "source": [
        "# Dossier source contenant les images\n",
        "SOURCE_DIR = \"./../data/Livrable1/Sketch\"\n",
        "DESTINATION_DIR = \"./../data/GAN_DATA\"\n",
        "\n",
        "def organize_images_by_extension(source_dir, destination_dir):\n",
        "    if not os.path.exists(source_dir):\n",
        "        print(f\"Le dossier {source_dir} n'existe pas.\")\n",
        "        return\n",
        "\n",
        "    for filename in os.listdir(source_dir):\n",
        "        file_path = os.path.join(source_dir, filename)\n",
        "\n",
        "        if os.path.isfile(file_path):\n",
        "            # Extraire l'extension\n",
        "            extension = filename.split('.')[-1].lower()\n",
        "\n",
        "            # Vérifier si c'est bien une image\n",
        "            if extension in ['jpg', 'jpeg', 'png', 'gif', 'bmp', 'tiff', 'webp']:\n",
        "                dest_dir = os.path.join(destination_dir, extension)\n",
        "                os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "                # Copier le fichier\n",
        "                shutil.copy(file_path, os.path.join(dest_dir, filename))\n",
        "                print(f\"Copié : {filename} -> {dest_dir}\")\n",
        "\n",
        "# organize_images_by_extension(SOURCE_DIR, DESTINATION_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IIXi1cilFPq"
      },
      "outputs": [],
      "source": [
        "# Charger les images\n",
        "def load_dataset(data_dir, IMG_SHAPE, batch_size, VALIDATION_SPLIT, SEED):\n",
        "    # Charger le dataset à partir des répertoires\n",
        "    train_set = tf.keras.utils.image_dataset_from_directory(\n",
        "        data_dir,\n",
        "        validation_split=VALIDATION_SPLIT,\n",
        "        subset='training',  \n",
        "        seed=SEED, \n",
        "        labels=None,\n",
        "        image_size=IMG_SHAPE,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "    )\n",
        "\n",
        "    test_set = tf.keras.utils.image_dataset_from_directory(\n",
        "        data_dir,\n",
        "        validation_split=VALIDATION_SPLIT,\n",
        "        subset='validation',  \n",
        "        seed=SEED,\n",
        "        labels=None,\n",
        "        image_size=IMG_SHAPE,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "    )\n",
        "    return train_set, test_set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fonctions création GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "54xoGoTglHdW"
      },
      "outputs": [],
      "source": [
        "def build_generator(LATENT_DIM, USE_BIAIS, IMG_HEIGHT, IMG_WIDTH):\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Dense(IMG_HEIGHT // 8 * IMG_WIDTH // 8 * 256, \n",
        "                     use_bias=USE_BIAIS, \n",
        "                     input_shape=(LATENT_DIM,)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        layers.Reshape((IMG_HEIGHT // 8, IMG_WIDTH // 8, 256)),\n",
        "\n",
        "        layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding=\"same\", use_bias=USE_BIAIS),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding=\"same\", use_bias=USE_BIAIS),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.LeakyReLU(),\n",
        "\n",
        "        layers.Conv2DTranspose(3, (5, 5), strides=(2, 2), padding=\"same\", activation=\"tanh\"),\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cGIEd5wlK-P"
      },
      "outputs": [],
      "source": [
        "def build_discriminator(IMG_SHAPE):\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Conv2D(64, (5, 5), strides=(2, 2), padding=\"same\", input_shape=IMG_SHAPE),\n",
        "        layers.LeakyReLU(),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\"),\n",
        "        layers.LeakyReLU(),\n",
        "        layers.Dropout(0.3),\n",
        "\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(1, activation=\"sigmoid\"),\n",
        "    ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYiEWUMwlNMO"
      },
      "outputs": [],
      "source": [
        "# Optimizers\n",
        "generator_optimizer = optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "discriminator_optimizer = optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n",
        "\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "# Fonction de perte\n",
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    return real_loss + fake_loss\n",
        "\n",
        "# Boucle d'entraînement\n",
        "@tf.function\n",
        "def train_step(images, generator, discriminator, BATCH_SIZE, LATENT_DIM):\n",
        "    noise = tf.random.normal([BATCH_SIZE, LATENT_DIM])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "        generated_images = generator(noise, training=True)\n",
        "\n",
        "        real_output = discriminator(images, training=True)\n",
        "        fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "        gen_loss = generator_loss(fake_output)\n",
        "        disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gen_gradients = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    disc_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gen_gradients, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(disc_gradients, discriminator.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss\n",
        "\n",
        "def train(dataset, generator, discriminator, BATCH_SIZE, LATENT_DIM, epochs=50):\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "        for image_batch in dataset:\n",
        "            train_step(image_batch, generator, discriminator, BATCH_SIZE, LATENT_DIM)\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyzPQ1vIqfl0"
      },
      "outputs": [],
      "source": [
        "def generate_and_show_images(generator, LATENT_DIM):\n",
        "    noise = tf.random.normal([16, LATENT_DIM])\n",
        "    generated_images = generator(noise, training=False)\n",
        "\n",
        "    fig, axes = plt.subplots(4, 4, figsize=(4, 4))\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        ax.imshow((generated_images[i] + 1) / 2)  # Dé-normalisation\n",
        "        ax.axis(\"off\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fonctions création VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Coding the specific sampling layer as a Keras Layer object\n",
        "class Sampling(layers.Layer):\n",
        "    \"\"\"\n",
        "    Create a custom sampling layer for the VAE.\n",
        "    This layer takes the mean and log variance of the latent space\n",
        "    and defines z as a random variable sampled from the normal distribution. \n",
        "    \"\"\"\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_logvar = inputs\n",
        "\n",
        "        nbatch = K.shape(z_mean)[0]\n",
        "        ndim = K.shape(z_mean)[1]\n",
        "\n",
        "        std = K.exp(z_logvar / 2)  # Correction: diviser par 2 pour obtenir l'écart-type\n",
        "        eps = K.random_normal(shape=(nbatch, ndim), mean=0., stddev=1.0)  # Correction: stddev=1.0\n",
        "\n",
        "        z = z_mean + eps * std\n",
        "\n",
        "        return z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_encoder_and_decoder(LATENT_DIM, IMG_SHAPE=(256, 256,)):\n",
        "    \n",
        "    \"\"\"\n",
        "    Create the encoder and decoder models for the VAE.\n",
        "    \"\"\"\n",
        "    # ------------------ Encoder -----------------\n",
        "    encoder_inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
        "    x = layers.Conv2D(32, (3, 3), strides=2, padding=\"same\")(encoder_inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    \n",
        "    x = layers.Conv2D(64, (3, 3), strides=2, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "        \n",
        "    x = layers.Conv2D(128, (3, 3), strides=2, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "\n",
        "\n",
        "    x = layers.Conv2D(256, (3, 3), strides=2, padding=\"same\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    \n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(300, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
        "    \n",
        "    z_mean = layers.Dense(LATENT_DIM, kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
        "    z_logvar = layers.Dense(LATENT_DIM, kernel_regularizer=tf.keras.regularizers.l2(1e-4))(x)\n",
        "    \n",
        "    encoder = tf.keras.Model(encoder_inputs, [z_mean, z_logvar], name=\"encoder\")\n",
        "    \n",
        "    \n",
        "    # ------------------ Decoder -------------------\n",
        "    IMG_HEIGHT, IMG_WIDTH, _ = IMG_SHAPE\n",
        "    decoder_inputs = tf.keras.Input(shape=(LATENT_DIM,))\n",
        "    \n",
        "    # Étendre le vecteur latent à une taille appropriée\n",
        "    decoder_hidden = layers.Dense((IMG_HEIGHT // 8) * (IMG_WIDTH // 8) * 256, activation=\"relu\")(decoder_inputs)\n",
        "    reshaped_hidden = layers.Reshape((IMG_HEIGHT // 8, IMG_WIDTH // 8, 256))(decoder_hidden)\n",
        "\n",
        "    # Reconstruction progressive de l'image\n",
        "    x = layers.Conv2DTranspose(256, (3, 3), strides=(2, 2), padding=\"same\")(reshaped_hidden)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    \n",
        "    x = layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU()(x)\n",
        "    x = layers.Dropout(0.25)(x)\n",
        "    \n",
        "    decoder_outputs = layers.Conv2DTranspose(3, (3, 3), strides=(2, 2), padding=\"same\", activation=\"sigmoid\")(x)\n",
        "    decoder_outputs = layers.Reshape((IMG_HEIGHT, IMG_WIDTH, 3))(decoder_outputs)\n",
        "\n",
        "    decoder = tf.keras.Model(decoder_inputs, decoder_outputs, name=\"decoder\")\n",
        "    \n",
        "    return encoder, decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class VAE_Autoencoder(tf.keras.Model):\n",
        "    def __init__(self, input_dim, encoder, decoder, beta=0.001):\n",
        "        super(VAE_Autoencoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.input_dim = input_dim\n",
        "        self.beta = beta  # Weight for the KL divergence loss\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Encode inputs to latent space\n",
        "        z_mean, z_logvar = self.encoder(inputs)\n",
        "        z = Sampling()([z_mean, z_logvar])  # Sample latent vector\n",
        "        reconstructed = self.decoder(z)  # Decode latent vector\n",
        "\n",
        "        # Reconstruction loss\n",
        "        reconstruction_loss = tf.keras.losses.binary_crossentropy(inputs, reconstructed)\n",
        "        reconstruction_loss = tf.reduce_sum(reconstruction_loss, axis=(1, 2))  # Sum over spatial dimensions\n",
        "        reconstruction_loss = tf.reduce_mean(reconstruction_loss)  # Average over the batch\n",
        "\n",
        "        # Clip z_mean and z_logvar for numerical stability\n",
        "        z_mean = tf.clip_by_value(z_mean, -5.0, 5.0)  # Clipping plus strict\n",
        "        z_logvar = tf.clip_by_value(z_logvar, -5.0, 5.0)  # Clipping plus strict\n",
        "\n",
        "        # KL divergence loss\n",
        "        epsilon = 1e-8  # Small constant for numerical stability\n",
        "        kl_loss = -0.5 * tf.reduce_sum(1 + z_logvar - tf.square(z_mean) - tf.exp(z_logvar + epsilon), axis=-1)\n",
        "        kl_loss = tf.reduce_mean(kl_loss)  # Average over the batch\n",
        "\n",
        "        # Total VAE loss\n",
        "        vae_loss = reconstruction_loss + self.beta * kl_loss\n",
        "        self.add_loss(vae_loss)  # Add the loss to the model\n",
        "\n",
        "        return reconstructed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Comparaison GAN vs VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16\n",
        "DATASET_PATH = \"./../data/GAN_DATA/jpg\"\n",
        "SEED = 42\n",
        "LATENT_DIM = 20  # Taille du bruit aléatoire\n",
        "VALIDATION_SPLIT = 0.2  # Pourcentage de données pour l'entraînement\n",
        "USE_BIAIS = True\n",
        "IMG_HEIGHT = 576 # real size : 583 - rounded to be divisible by 8\n",
        "IMG_WIDTH = 408 # real size : 411 - rounded to be divisible by 8\n",
        "IMG_SHAPE = (IMG_HEIGHT, IMG_WIDTH, 3)\n",
        "\n",
        "GAN_EPOCHS = 100\n",
        "VAE_EPOCHS = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Création du dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Création du dataset TensorFlow\n",
        "\n",
        "train_dataset, test_dataset = load_dataset(\n",
        "    data_dir=DATASET_PATH,\n",
        "    IMG_SHAPE=IMG_SHAPE[:2],  # Pass only height and width\n",
        "    batch_size=BATCH_SIZE,\n",
        "    VALIDATION_SPLIT=VALIDATION_SPLIT,\n",
        "    SEED=SEED,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.map(lambda x: x / 255.0)  # Normalize to [0, 1]\n",
        "test_dataset = test_dataset.map(lambda x: x / 255.0)  # Normalize to [0, 1]\n",
        "\n",
        "for dataset in [train_dataset, test_dataset]:\n",
        "    for batch in dataset.take(1):\n",
        "        min_val = tf.reduce_min(batch).numpy()\n",
        "        max_val = tf.reduce_max(batch).numpy()\n",
        "        \n",
        "        assert min_val >= 0 and max_val <= 1, \"Min and Max values are not in the range [0, 1]\"\n",
        "        print(\"Test Dataset - Min:\", min_val, \"Max:\", max_val)    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# prefetch\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_dataset = train_dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Création du GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generator = build_generator(LATENT_DIM, USE_BIAIS, IMG_HEIGHT, IMG_WIDTH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "discriminator = build_discriminator(IMG_SHAPE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train(train_dataset, \n",
        "      epochs=GAN_EPOCHS, \n",
        "      generator=generator, \n",
        "      discriminator=discriminator, \n",
        "      BATCH_SIZE=BATCH_SIZE, \n",
        "      LATENT_DIM=LATENT_DIM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate_and_show_images(generator=generator, LATENT_DIM=LATENT_DIM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Création du VAE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Create encoder and decoder models for the VAE\n",
        "\"\"\"\n",
        "encoder, decoder = create_encoder_and_decoder(LATENT_DIM, IMG_SHAPE)\n",
        "plot_model(encoder, to_file='./figures/encoder.png', show_shapes=True)\n",
        "plot_model(decoder, to_file='./figures/decoder.png', show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder.summary()\n",
        "decoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "vae = VAE_Autoencoder(input_dim=IMG_SHAPE, \n",
        "                      encoder=encoder, \n",
        "                      decoder=decoder, \n",
        "                      beta=0.001)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, clipnorm=1.0)  # Ajout de gradient clipping\n",
        "vae.compile(optimizer=optimizer, loss=None)\n",
        "vae.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=10,\n",
        "        verbose=1,\n",
        "        mode='min',\n",
        "        restore_best_weights=True\n",
        "    ),\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath='./models/vae_model.keras',\n",
        "        save_best_only=True,\n",
        "        monitor='val_loss',\n",
        "        mode='min'\n",
        "    ),]\n",
        "\n",
        "history_vae = vae.fit(\n",
        "    train_dataset,\n",
        "    epochs=VAE_EPOCHS,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    validation_data=test_dataset,\n",
        "    verbose=1,\n",
        "    callbacks=callbacks,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "img = vae.predict(\n",
        "    test_dataset.take(1),\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "# Plotting images\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(img[1])\n",
        "plt.axis(\"off\")\n",
        "    \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_images = test_dataset.take(1)\n",
        "for batch in test_images:\n",
        "    test_images = batch.numpy()\n",
        "    break\n",
        "vae_images = vae.predict(test_images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crée une figure avec trois sous-grilles de 4x4\n",
        "fig, axes = plt.subplots(2, 9, figsize=(12, 6))\n",
        "\n",
        "# Affiche les images du test dataset dans la première sous-grille\n",
        "for i, ax in enumerate(axes[0]):\n",
        "    ax.imshow(test_images[i], cmap='gray')\n",
        "    ax.axis('off')\n",
        "    if i == 0:\n",
        "        ax.set_title('Test Dataset')\n",
        "\n",
        "# Affiche les images générées par le VAE dans la deuxième sous-grille\n",
        "for i, ax in enumerate(axes[1]):\n",
        "    ax.imshow(vae_images[i], cmap='gray')\n",
        "    ax.axis('off')\n",
        "    if i == 0:\n",
        "        ax.set_title('VAE Images')\n",
        "\n",
        "# Ajuste l'espacement entre les sous-grilles\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generate_and_show_images(generator=generator, LATENT_DIM=LATENT_DIM)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
